{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get comments from Reddit",
   "id": "e17f442aa2336fcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "b2b2c6f15f91aad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "from datetime import datetime as dt  # imports datetime to deal with dates\n",
    "\n",
    "import pandas as pd  # imports pandas for data manipulation\n",
    "import praw  # imports praw for reddit access\n",
    "from dotenv import load_dotenv  # get login secrets\n",
    "from openai import OpenAI\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments, Comment, Subreddit\n",
    "from tqdm import tqdm  # show progress bars\n",
    "\n",
    "load_dotenv()  # gets secrets"
   ],
   "id": "c8c9ea9eb6891acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Helper function to get all comments from a post",
   "id": "ce39cfd5fec820f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_comments(comments_list: list[Comment], limit: int=None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Gets all comments from a list, even if there are MoreComments\n",
    "    :param comments_list: The list of comments\n",
    "    :param limit: Optional - the limit of MoreComments to read, or None for no limit\n",
    "    :return: A list of all comments, with only Comments\n",
    "    \"\"\"\n",
    "    all_comments: list[str | Comment] = []\n",
    "    more_comments_count: int = 0\n",
    "\n",
    "    for item in comments_list:\n",
    "        if isinstance(item, MoreComments):\n",
    "            if limit is None or more_comments_count < limit:\n",
    "                try:\n",
    "                    # Get comments from MoreComments object\n",
    "                    new_comments: list[Comment] = item.comments()\n",
    "\n",
    "                    # Recursively process in case there are nested MoreComments\n",
    "                    processed_comments = get_all_comments(new_comments, limit)\n",
    "                    all_comments.extend(processed_comments)\n",
    "\n",
    "                    more_comments_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error expanding MoreComments: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            # Regular Comment object\n",
    "            all_comments.append(item)\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "\n",
    "# log into reddit api\n",
    "reddit: Reddit = praw.Reddit(client_id=os.getenv(\"CLIENT_ID\"),\n",
    "                     client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "                     user_agent='data collector for u/Delicious-Corner6100',\n",
    "                     username=os.getenv(\"REDDIT_USERNAME\"),\n",
    "                     password=os.getenv(\"REDDIT_PASSWORD\"))\n",
    "\n",
    "# make sure that we are logged in correctly\n",
    "if reddit.user.me() != os.getenv(\"REDDIT_USERNAME\"):\n",
    "    print(\"Failed to log in to Reddit :(\")\n",
    "    sys.exit(1)\n",
    "print(f\"Logged in to Reddit as {os.getenv(\"REDDIT_USERNAME\")}\")\n",
    "\n",
    "# create a DataFrame to store posts\n",
    "# posted date in epoch, comments as list, everything else is a string\n",
    "posts: pd.DataFrame = pd.DataFrame(columns=[\"Posted Time\",\n",
    "                              \"Title\",\n",
    "                              \"Author\",\n",
    "                              \"Link\",\n",
    "                              \"Content\",\n",
    "                              \"Comments\"])\n",
    "\n",
    "# list subreddits to search\n",
    "# set subreddits = [\"all\"] to search all of reddit\n",
    "# skipcq: FLK-E501\n",
    "# subreddits = ['cybersecurity', 'technology', 'k12sysadmin', 'toronto', 'canada', 'askTO', 'raleigh', 'linustechtips']\n",
    "subreddits: list[str] = [\"all\"]\n",
    "\n",
    "# look through every subreddit\n",
    "for subreddit_name in subreddits:\n",
    "    # get the subreddit via api\n",
    "    subreddit: Subreddit = reddit.subreddit(subreddit_name)\n",
    "    # get the messages that meet a query (the same as the search bar)\n",
    "    # sorts by relevance and gets only messages from the most recent year\n",
    "    # change limit to the most appropriate limit\n",
    "    # tqdm shows progress bar\n",
    "    for post in tqdm(subreddit.search(\"powerschool data breach\",\n",
    "                                      sort=\"relevance\",\n",
    "                                      time_filter=\"year\",\n",
    "                                      limit=50),\n",
    "                     desc=f\"r/{subreddit} progress\"):\n",
    "        # check the time of the post\n",
    "        # 12/28/2024, the date of the breach, in epoch time\n",
    "        breach_time: int = 1735344000\n",
    "        if post.created_utc <= breach_time:\n",
    "            # do not process comments if they occur before the breach\n",
    "            continue\n",
    "        time = dt.fromtimestamp(post.created_utc)\n",
    "\n",
    "        # gets the link so we can review the post\n",
    "        # skipcq FLK-E501\n",
    "        link: str = f\"https://www.reddit.com/r/{post.subreddit.display_name}/comments/{post.id}/\"\n",
    "\n",
    "        # collects the data necessary\n",
    "        # replace newlines with spaces because they are easier to work with\n",
    "        data: list[str] = [time,\n",
    "                           post.title,\n",
    "                           post.author.name,\n",
    "                           link,\n",
    "                           post.selftext.replace(\"\\n\", \" \"),\n",
    "                           post.comments.list()]\n",
    "        # adds the data to the dataframe\n",
    "        posts.loc[len(posts)] = data\n",
    "\n",
    "# First, expand all MoreComments objects in all comment lists\n",
    "expanded_comments_lists = []\n",
    "\n",
    "for comment_list in tqdm(posts[\"Comments\"], desc=\"Expanding MoreComments\"):\n",
    "    expanded_list: list[str] = get_all_comments(comment_list)\n",
    "    expanded_comments_lists.append(expanded_list)\n",
    "\n",
    "# Now find the maximum number of comments after expansion\n",
    "num_comments: int = 0\n",
    "for expanded_comment_list in expanded_comments_lists:\n",
    "    num_comments: int = (len(expanded_comment_list)\n",
    "                    if len(expanded_comment_list) > num_comments\n",
    "                    else num_comments)\n",
    "\n",
    "# Create columns in the dataframe based on the actual expanded comment counts\n",
    "comments: pd.DataFrame = pd.DataFrame(\n",
    "    columns=[f\"Comment{i}\" for i in range(1, num_comments + 1)]\n",
    ")\n",
    "\n",
    "# Process the already-expanded comment lists\n",
    "for expanded_comment_list in tqdm(expanded_comments_lists, desc=\"Processing comments\"):\n",
    "    text: list[str | None] = []\n",
    "\n",
    "    # Get every comment in every post (already expanded)\n",
    "    for comment in expanded_comment_list:\n",
    "        # Replace newlines with spaces because they are easier to work with\n",
    "        text.append(comment.body.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Fill any empty spaces with None\n",
    "    text.extend([None] * (num_comments - len(text)))\n",
    "\n",
    "    # Add it to the dataframe\n",
    "    comments.loc[len(comments)] = text\n",
    "\n",
    "# merge the two tables together\n",
    "res: pd.DataFrame = pd.concat([posts, comments], axis=1)\n",
    "# delete the list of comments because they have been converted to strings\n",
    "res: pd.DataFrame = res.drop(\"Comments\", axis=1)\n",
    "\n",
    "# write the final dataframe to a csv\n",
    "res.to_csv(\"posts.csv\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ollama AI",
   "id": "144ceadd8943a2b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "def ollama_ai(prompt: str, model: str = \"gemma3:12b\") -> str | None:\n",
    "    \"\"\"\n",
    "    Makes a request to a local ollama server to run an AI query.\n",
    "    :param prompt: String - the query for the AI.\n",
    "    :param model: String - the model for ollama to run. Default is Gemma 3 1b.\n",
    "    :return: A JSON object with response data from the API.\n",
    "    \"\"\"\n",
    "    # url of the server\n",
    "    url: str = \"http://jacobs-ubuntu:11434/api/generate\"\n",
    "\n",
    "    req_data: dict[str, str | bool] = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # makes a request to the ollama server\n",
    "        response = requests.post(url, json=req_data)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None"
   ],
   "id": "f30f8ebd68bbeabf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Google AI",
   "id": "4e50cbdead134132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from google import genai  # google ai package\n",
    "from google.ai.generativelanguage_v1 import GenerateContentResponse\n",
    "from google.api_core import exceptions\n",
    "from google.genai import Client\n",
    "\n",
    "def google_ai(prompt: str, api_key: str, model: str = \"gemma3\") -> str | None:\n",
    "    \"\"\"\n",
    "    Calls the Google/Gemini api to get their AI's answers\n",
    "    :param prompt: The prompt to ask the AI\n",
    "    :param model: Which AI model to ask - default is gemma 3\n",
    "    :param api_key: Google api key\n",
    "    :return: String response of the AI\n",
    "    \"\"\"\n",
    "    # get google gen ai client object\n",
    "    client: Client = genai.Client(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        # get response from api and return it\n",
    "        response: GenerateContentResponse = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=prompt,\n",
    "        )\n",
    "\n",
    "        return response.text\n",
    "    except exceptions.ResourceExhausted as e:\n",
    "        print(f\"{e}\\nAPI limit reached. Taking a break for a few seconds...\")\n",
    "        sleep(15)\n",
    "        return None"
   ],
   "id": "1b0e20e71558bea5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI API",
   "id": "57bfdffe939c882b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def openai_ai(prompt: str, api_key: str, model: str = \"o4-mini\", instructions: str = \"\") -> str | None:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI api to get their AI's response.\n",
    "    :param prompt: The prompt to ask the AI.\n",
    "    :param model: The model to use. Defaults to o4-mini\n",
    "    :param api_key: The OpenAI API key. Default to the value set in .env\n",
    "    :param instructions: The instructions that the model will receive.\n",
    "    :return: String response from the AI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client: OpenAI = OpenAI(\n",
    "            api_key=api_key\n",
    "        )\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            instructions=instructions,\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response.output_text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ],
   "id": "a195fee417c4b856"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labeling",
   "id": "84ad62dfd26b4810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initially label messages\n",
    "posts: pd.DataFrame = pd.read_csv(\"roles.csv\")\n",
    "\n",
    "# get all quotes into a list\n",
    "quotes: list[str] = posts[\"quote\"].values.flatten().tolist()\n",
    "responses: list[str] = []\n",
    "\n",
    "for quote in tqdm(quotes, desc=\"AI Progress\"):\n",
    "    #ai_response: str | None = openai_ai(\n",
    "    #    prompt=\"Below is a quote about the PowerSchool data breach. \"\n",
    "    #           \"Label the data based on what was included. \"\n",
    "    #           \"For example, if the author of the message complained about the delay in notification, you can label the message as 'lack of communication'. \"\n",
    "    #           \"Additionally, if the author of the message worried about their children's data, you can label the message as 'worried about child data'. \"\n",
    "    #           \"You can label messages with as many labels as necessary and you may generate as many labels as you need. \"\n",
    "    #           f\"Message: {quote}\",\n",
    "    #    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    #)\n",
    "\n",
    "    ai_response: str | None = openai_ai(\n",
    "        prompt=\"Below is a quote about the PowerSchool data breach. \"\n",
    "               \"Label the data based on what was included. Respond with a list of labels that fit the message. \"\n",
    "               \"You can select as many labels as necessary, but you may not add labels that are not on this list: \"\n",
    "               \"Lack of communication - This label means the author was not notified about the breach before they posted. \"\n",
    "               \"Bad communication - This label means the author was notified about the breach before they posted, but they were still unsure about some aspect of the breach. \"\n",
    "               \"Worried about data - This label means the author was worried about the impact of this data breach. For example, posters could be worried about identity theft or fraud. \"\n",
    "               \"Not worried about data - This label means the author was not concerned about the data breach. For example, posters may assume that their data is already on the dark web, so this breach didn't affect them. \"\n",
    "               \"Issues with remediation - This label means the author had problems when trying to protect themselves. For example, they may have encountered issues when signing up for credit monitoring. \"\n",
    "               \"Feeling of inevitability - This label means the author felt that they had no other choice but to give data to insecure companies. \"\n",
    "               \"Insufficient remedies - This label means the author felt that actions taken after the breach were not enough to fully protect themselves. \"\n",
    "               \"Confused - This label means that the author did not know what to do after the data breach to protect themselves. \"\n",
    "               \"Surprised - This label means that the author was surprised that PowerSchool was breached. \"\n",
    "               \"Lost trust - This label means that the author no longer trusts companies with their data after the breach. \"\n",
    "               \"Lack of funding - This label means that the author believes that schools do not have enough money to afford better systems. \"\n",
    "               \"Outdated technology - This label means that the author believes that schools' technologies are so old they cause security vulnerabilities. \"\n",
    "               \"High stress - This label means that the author believes that administrators and IT professionals do not have the ability to protect their systems due to high workloads. \"\n",
    "               \"Lack of accountability - This label means that the author feels like PowerSchool was not adequately punished for the data breach. \"\n",
    "               f\"Message: {quote}\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    responses.append(ai_response)\n",
    "\n",
    "posts[\"labels\"] = responses\n",
    "posts.to_csv(\"labels.csv\", index=False)"
   ],
   "id": "ed2f1869d3a8dee7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Role selection",
   "id": "a8c75e926e10653a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# open file with comments\n",
    "posts: pd.DataFrame = pd.read_csv(\"posts.csv\")\n",
    "# get the number of comments\n",
    "num_comments: int = int(posts.columns[-1][7:])\n",
    "\n",
    "column_names: list[str] = [f\"Comment{i}\"\n",
    "                for i in range(1, num_comments + 1)] + [\"Content\"]\n",
    "\n",
    "# get all quotes into a list\n",
    "quotes: list[str] = posts[column_names].values.flatten().tolist()\n",
    "# remove any empty values or comments that were deleted\n",
    "quotes = [x for x in quotes if (type(x) is str and x != '[deleted]')]\n",
    "\n",
    "results: list[str] = []\n",
    "for quote in tqdm(quotes, desc=\"AI Progress\"):\n",
    "    # call the openai api to determine role\n",
    "    result: str | None = openai_ai(\n",
    "        prompt=\"Read the following message about the PowerSchool data breach and determine the role of the person who wrote it. \"\n",
    "               \"The author has one of the following roles: parent, student, teacher, admin. \"\n",
    "               \"The first word of your response should be the role of the person. \"\n",
    "               \"Provide an explanation for why you chose the role for that message. \"\n",
    "               \"If you are not more than 50% confident in your classification, respond with unsure. \"\n",
    "               \"Parents generally talk about their children, using words such as 'my son', 'my daughter', 'my child(ren)', etc. \"\n",
    "               \"They also mention receiving messages from school boards about the data breach. \"\n",
    "               \"Students have generally graduated since we can't collect data from people under 18. \"\n",
    "               \"They usually mention that they graduated some years ago. \"\n",
    "               \"Teachers usually mention that they have experience teaching. \"\n",
    "               \"Administrators usually have more technical knowledge, but that is not a guaranteed factor. \"\n",
    "               \"They talk with PowerSchool directly and manage a school's or district's powerschool instance. \"\n",
    "               \"Note that administrators must be from a K-12 school, so postsecondary admins, admins that worked on tech other than PowerSchool, and PowerSchool employees should be classified as general. \"\n",
    "               \"Additionally, simply knowing technical terms does not automatically make them an administrator. \"\n",
    "               \"They need to have experience working in a K-12 school's IT department. \"\n",
    "               \"If the person does not fit any of the labels, respond with the best fit label that you can think of. \"\n",
    "               \"If the message does not relate to the PowerSchool data breach, respond with not relevant. \"\n",
    "               \"Message: \"\n",
    "               f\"{quote}\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    if result:\n",
    "        # add the result so we can track it\n",
    "        results.append(result)\n",
    "\n",
    "# put the quote to the role and save as csv\n",
    "out: pd.DataFrame = pd.DataFrame({\n",
    "    \"quote\": quotes,\n",
    "    \"role\": results\n",
    "})\n",
    "\n",
    "out.to_csv(\"roles.csv\", index=False)"
   ],
   "id": "c74bba015851834a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean labels",
   "id": "5f94539fec0c8744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "df = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "def clean_and_capitalize(text):\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Remove everything except letters and spaces\n",
    "        letters_only = re.sub(r'[^a-zA-Z\\s]', '', line).replace(\"â€¢\", \"\").replace(\"-\", \"\")\n",
    "        # Collapse multiple spaces, strip, then capitalize first letter\n",
    "        cleaned_line = re.sub(r'\\s+', ' ', letters_only).strip().capitalize()\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "def normalize_labels(text):\n",
    "    # Split by commas, clean each label\n",
    "    parts = [part.strip().strip('\"').strip(\"'\").lower() for part in text.split(',')]\n",
    "    return ', '.join(parts)\n",
    "\n",
    "\n",
    "df[\"role\"] = df[\"role\"].str.split().str[0].apply(clean_and_capitalize).apply(normalize_labels)\n",
    "\n",
    "df.to_csv(\"labels_cleaned.csv\", index=False)"
   ],
   "id": "6070a6c9dd62ce3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Format labels",
   "id": "9af47263deedcb72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"labels_formatted.csv\")\n",
    "\n",
    "def normalize_labels(text):\n",
    "    # Split by commas, clean each label\n",
    "    parts = [part.strip().strip('\"').strip(\"'\").lower() for part in text.split(',')]\n",
    "    return ', '.join(parts)\n",
    "\n",
    "df['Labels'] = df['Labels'].apply(normalize_labels)\n",
    "df_grouped = df.groupby('Labels', as_index=False).sum(numeric_only=True)\n",
    "\n",
    "df_grouped.to_csv(\"labels_formatted.csv\", index=False)"
   ],
   "id": "12a4eefea1741e6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Also clean labels",
   "id": "7ec7d2aa81f8f103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"labels.csv\")  # Replace with your filename\n",
    "\n",
    "# Choose the column to clean\n",
    "column_name = \"labels\"  # Replace with your column name\n",
    "\n",
    "# Function to clean each value\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'[^a-zA-Z,\\n ]', '', str(text))\n",
    "\n",
    "# Apply the cleaning function to the column\n",
    "df[column_name] = df[column_name].apply(clean_text)\n",
    "\n",
    "# Optional: save to a new CSV\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)"
   ],
   "id": "bf0c51a7bbcd6b73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Co-occurance Heatmap Generation",
   "id": "c5770630ffca4b3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Example input: list of strings with newline-separated labels (possibly with extra whitespace)\n",
    "data = pd.read_csv(\"cleaned_file.csv\")[\"labels\"]\n",
    "\n",
    "cleaned_entries = []\n",
    "for entry in data:\n",
    "    if pd.isna(entry):\n",
    "        continue\n",
    "    # Replace both newlines and commas with a common separator (e.g. newline), then split\n",
    "    raw_labels = entry.replace(',', '\\n').split('\\n')\n",
    "    labels = [label.strip() for label in raw_labels if label.strip()]\n",
    "    if len(labels) > 1:\n",
    "        cleaned_entries.append(labels)\n",
    "\n",
    "# Count co-occurrences\n",
    "co_counts = defaultdict(int)\n",
    "all_labels = set()\n",
    "\n",
    "for labels in cleaned_entries:\n",
    "    unique_labels = sorted(set(labels))  # sort for consistent pair order\n",
    "    all_labels.update(unique_labels)\n",
    "    for a, b in combinations(unique_labels, 2):\n",
    "        co_counts[(a, b)] += 1\n",
    "        co_counts[(b, a)] += 1  # make it symmetric\n",
    "\n",
    "# Initialize co-occurrence matrix\n",
    "all_labels = sorted(all_labels)\n",
    "co_matrix = pd.DataFrame(0, index=all_labels, columns=all_labels)\n",
    "\n",
    "for (a, b), count in co_counts.items():\n",
    "    co_matrix.at[a, b] = count\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_matrix, annot=True, fmt='d', cmap='YlGnBu', square=True, cbar_kws={'label': 'Co-occurrence'})\n",
    "plt.title(\"Label Co-occurrence Heatmap\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cooccurrence_heatmap.png\", dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "4a61da286eaa0b3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3c84ddb0a34d2b1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
